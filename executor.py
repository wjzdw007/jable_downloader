import re
import time
import random

import config
import model_crawler
import utils
import video_crawler
import hot_crawler

CONF = config.CONF


def _add_subscription(input_urls):
    cur_subscription = []
    for input_url in input_urls:
        model_crawler.input_url_validator(input_url)
        name, _ = model_crawler.get_model_names_and_last_page_num(input_url)
        cur_subscription.append({'url': input_url, 'name': name})

    all_subs = CONF.get('subscriptions', [])

    for subs in all_subs:
        url_group = set()
        for sub_info in subs:
            url_group.add(sub_info['url'])
        if set(input_urls) == url_group:
            print("cur_subscription %s already exists." % subs)
            return

    all_subs.append(cur_subscription)
    CONF['subscriptions'] = all_subs
    config.update_config(CONF)
    print("add cur_subscription success.")
    print(cur_subscription)


def get_need_sync_video_ids(sub):
    # first update cache
    cache_info = utils.get_video_ids_map_from_cache()
    try:
        for item in sub:
            url = item['url']
            cached_video_ids = set(cache_info.get(url, set())) if cache_info else set()
            remote_video_id_set = model_crawler.get_all_video_ids(url, cached_video_ids)
            cache_info[url] = list(remote_video_id_set)
    except Exception:
        raise
    finally:
        utils.update_video_ids_cache(cache_info)

    need_sync_video_ids = set(cache_info[sub[0]['url']])
    for item in sub[1:]:
        url = item['url']
        need_sync_video_ids &= set(cache_info.get(url))
    return need_sync_video_ids


def print_all_subs(all_subs, print_url=False):
    if not all_subs:
        print("å½“å‰æ— ä»»ä½•è®¢é˜…å†…å®¹")
        return

    print("å½“å‰å…±%sä¸ªè®¢é˜…ï¼Œå†…å®¹å¦‚ä¸‹:" % len(all_subs))
    for index, subs in enumerate(all_subs):
        names = '-'.join([foo['name'] for foo in subs])
        if not print_url:
            print("%s\t: è®¢é˜…å: %s " % (index+1, names))
        else:
            urls = [foo['url'] for foo in subs]
            print("%s\t: è®¢é˜…å: %s\t\tè®¢é˜…é“¾æ¥: %s" % (index+1, names, urls))


def process_subscription(args):
    if args.add:
        input_urls = args.add
        _add_subscription(input_urls)

    elif args.get:
        all_subs = CONF.get('subscriptions', [])
        print_all_subs(all_subs, print_url=True)
    elif args.sync_videos:
        all_subs = CONF.get('subscriptions', [])
        output_path = CONF.get("outputDir", './')
        if args.ids:
            all_subs = [all_subs[i-1] for i in args.ids if (1 <= i <= len(all_subs))]
        local_video_id_set = utils.get_local_video_list(path=output_path)
        block_video_ids = {str.lower(video_id) for video_id in config.CONF.get("videoIdBlockList", [])}
        ignore_video_ids = local_video_id_set | block_video_ids
        base_url = "https://jable.tv/videos/"

        download_inerval = CONF.get("downloadInterval", 1)
        print_all_subs(all_subs)

        for subs in all_subs:
            subs_name = '-'.join([foo['name'] for foo in subs])
            print("\n===================================")
            print("  åŒæ­¥è®¢é˜…:\t%s" % subs_name)
            print("===================================\n")
            remote_video_id_set = get_need_sync_video_ids(subs)
            need_sync_video_ids = remote_video_id_set - ignore_video_ids
            need_sync_number = len(need_sync_video_ids)
            print("è¯¥è®¢é˜…è¿œç«¯ %s ä¸ª / æœ¬åœ°å·²å­˜åœ¨ %s ä¸ª " %
                  (len(remote_video_id_set), len(remote_video_id_set & ignore_video_ids)))

            # å°† set è½¬æ¢ä¸º list å¹¶éšæœºæ‰“ä¹±é¡ºåº
            need_sync_video_list = list(need_sync_video_ids)
            random.shuffle(need_sync_video_list)
            print("ğŸ”€ å·²éšæœºæ‰“ä¹±ä¸‹è½½é¡ºåº")

            print("å¼€å§‹åŒæ­¥ %s çš„è¿œç«¯è§†é¢‘åˆ°æœ¬åœ°..." % '-'.join([foo['name'] for foo in subs]))

            for index, video_id in enumerate(need_sync_video_list):
                print("\nè¯¥è®¢é˜…éœ€åŒæ­¥è§†é¢‘ %s ä¸ª / å‰©ä½™ %s ä¸ª " % (need_sync_number, need_sync_number - index))
                download_url = base_url + video_id + '/'
                # print(download_url)
                video_crawler.download_by_video_url(download_url)

                ignore_video_ids.add(video_id)
                if index < len(need_sync_video_list) - 1:
                    time.sleep(download_inerval)

            print("è®¢é˜… %s åŒæ­¥å®Œæˆ" % subs_name)
        print("\n==æ‰€æœ‰è®¢é˜…åŒæ­¥å®Œæˆ==\n")


def process_videos(args):
    video_urls = []
    for url in args.urls:
        if "videos" not in url:
            raise Exception("only support video url")
        if not url.endswith('/'):
            video_urls.append(url+'/')
        else:
            video_urls.append(url)

    output_path = CONF.get("outputDir", './')
    local_video_id_set = utils.get_local_video_list(path=output_path)
    block_video_ids = {str.lower(video_id) for video_id in config.CONF.get("videoIdBlockList", [])}
    ignore_video_ids = local_video_id_set | block_video_ids

    # ä¿®æ­£æ­£åˆ™ï¼šåŒ¹é…å®Œæ•´çš„è§†é¢‘ IDï¼ŒåŒ…æ‹¬æ‰€æœ‰åç¼€ï¼ˆå¦‚ -c, -cn ç­‰ï¼‰
    re_extractor = re.compile(r"[a-zA-Z0-9]{2,}-\d{3,}(?:-[a-zA-Z0-9]+)?")

    for video_url in video_urls:
        re_res = re_extractor.search(video_url)
        if re_res:
            video_id = re_res.group(0)
            if video_id and video_id in ignore_video_ids:
                print("è§†é¢‘ %s å·²ç»ä¸‹è½½ï¼Œè·³è¿‡è¯¥è§†é¢‘" % video_url)
                continue
            ignore_video_ids.add(video_id)
        video_crawler.download_by_video_url(video_url)


def process_hot(args):
    """
    å¤„ç†çƒ­é—¨è§†é¢‘ä¸‹è½½å‘½ä»¤

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
            - top: ä¸‹è½½æ•°é‡ï¼ˆé»˜è®¤ 4ï¼‰
            - min_likes: æœ€å°ç‚¹èµæ•°ï¼ˆé»˜è®¤ 2000ï¼‰
    """
    top_n = args.top if hasattr(args, 'top') and args.top else 4
    min_likes = args.min_likes if hasattr(args, 'min_likes') and args.min_likes else 2000

    hot_crawler.download_hot_videos(top_n=top_n, min_likes=min_likes)
