#!/usr/bin/env python3
"""
çƒ­é—¨å½±ç‰‡æ•°æ®çˆ¬è™«æ¨¡å—
è´Ÿè´£ä» https://jable.tv/hot/ çˆ¬å–è§†é¢‘æ•°æ®å’Œæ¼”å‘˜ä¿¡æ¯
"""

import re
import time
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from progress_tracker import ProgressTracker

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
WHITESPACE_PATTERN = re.compile(r'\s+')
PAGE_NUMBER_PATTERN = re.compile(r'/hot/(\d+)/')

# ä½¿ç”¨ä¼˜åŒ–ç‰ˆçš„ utilsï¼ˆå¤ç”¨æµè§ˆå™¨ï¼Œç¦ç”¨èµ„æºï¼Œç§»é™¤å›ºå®šç­‰å¾…ï¼‰
try:
    import utils_fast
    USE_FAST_MODE = True
    print("â„¹ï¸  ä½¿ç”¨ä¼˜åŒ–ç‰ˆçˆ¬è™«ï¼ˆutils_fastï¼‰")

    def fetch_page(url: str, retry: int = 3) -> str:
        """ç»Ÿä¸€çš„é¡µé¢è·å–æ¥å£ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        return utils_fast.fast_requests_get(url, retry)

    def cleanup_browser():
        """æ¸…ç†æµè§ˆå™¨å®ä¾‹"""
        utils_fast.close_browser_instance()

except ImportError:
    import utils
    USE_FAST_MODE = False
    print("âš ï¸  ä¼˜åŒ–ç‰ˆä¸å¯ç”¨ï¼Œä½¿ç”¨åŸç‰ˆ utils")

    def fetch_page(url: str, retry: int = 3) -> str:
        """ç»Ÿä¸€çš„é¡µé¢è·å–æ¥å£ï¼ˆåŸç‰ˆï¼‰"""
        return utils.scrapingant_requests_get(url, retry)

    def cleanup_browser():
        """æ¸…ç†æµè§ˆå™¨å®ä¾‹ï¼ˆåŸç‰ˆæ— éœ€æ¸…ç†ï¼‰"""
        pass


def extract_videos_from_page(html: str) -> List[Dict]:
    """
    ä»é¡µé¢ HTML ä¸­æå–è§†é¢‘ä¿¡æ¯

    Args:
        html: é¡µé¢ HTML å†…å®¹

    Returns:
        è§†é¢‘åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {video_id, title, views, likes, url}
    """
    # ä½¿ç”¨ lxml è§£æå™¨ï¼ˆæ¯” html.parser å¿«5-10å€ï¼‰
    try:
        soup = BeautifulSoup(html, 'lxml')
    except:
        # å¦‚æœ lxml æœªå®‰è£…ï¼Œå›é€€åˆ° html.parser
        soup = BeautifulSoup(html, 'html.parser')
    video_containers = soup.select('div.video-img-box')

    videos = []

    for container in video_containers:
        try:
            # æå–è§†é¢‘é“¾æ¥å’Œ ID
            link_tag = container.select_one('a[href*="/videos/"]')
            if not link_tag:
                continue

            video_url = link_tag.get('href', '')
            if not video_url:
                continue

            # ä» URL ä¸­æå–è§†é¢‘ ID
            video_id = video_url.split('/')[-2] if '/' in video_url else ''
            if not video_id:
                continue

            # æå–æ ‡é¢˜
            title_tag = container.select_one('h6.title a')
            title = title_tag.get_text(strip=True) if title_tag else ''

            # æå–ç»Ÿè®¡æ•°æ®ï¼ˆè§‚çœ‹æ•°å’Œç‚¹èµæ•°ï¼‰
            sub_title = container.select_one('p.sub-title')

            views = 0
            likes = 0

            if sub_title:
                text = sub_title.get_text()

                # æŒ‰è¡Œåˆ†å‰²ï¼Œæå–æ¯è¡Œçš„æ•°å­—
                lines = text.strip().split('\n')
                numbers = []

                for line in lines:
                    line = line.strip()
                    if line:
                        # å»é™¤æ‰€æœ‰ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
                        num_str = WHITESPACE_PATTERN.sub('', line)
                        if num_str.isdigit():
                            numbers.append(int(num_str))

                # ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯è§‚çœ‹æ•°ï¼Œç¬¬äºŒä¸ªæ˜¯ç‚¹èµæ•°
                if len(numbers) >= 2:
                    views = numbers[0]
                    likes = numbers[1]

            videos.append({
                'video_id': video_id,
                'title': title,
                'views': views,
                'likes': likes,
                'url': video_url
            })

        except Exception as e:
            print(f"âš ï¸  è§£æè§†é¢‘å¤±è´¥: {e}")
            continue

    return videos


def get_total_pages(html: str) -> int:
    """
    ä»é¡µé¢ HTML ä¸­è·å–æ€»é¡µæ•°

    Args:
        html: é¡µé¢ HTML å†…å®¹

    Returns:
        æ€»é¡µæ•°
    """
    # ä½¿ç”¨ lxml è§£æå™¨
    try:
        soup = BeautifulSoup(html, 'lxml')
    except:
        soup = BeautifulSoup(html, 'html.parser')

    # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥
    pagination_links = soup.select('ul.pagination li a[href*="/hot/"]')
    max_page = 1

    for link in pagination_links:
        # æ£€æŸ¥æ˜¯å¦æ˜¯"æœ€å¾Œ"é“¾æ¥
        text = link.get_text(strip=True)
        href = link.get('href', '')

        # å¦‚æœæ˜¯"æœ€å¾Œ"é“¾æ¥ï¼Œä¼˜å…ˆä½¿ç”¨
        if 'æœ€å¾Œ' in text or 'Â»' in text:
            match = PAGE_NUMBER_PATTERN.search(href)
            if match:
                return int(match.group(1))

        # å¦åˆ™ï¼Œè®°å½•æœ€å¤§é¡µç 
        match = PAGE_NUMBER_PATTERN.search(href)
        if match:
            page_num = int(match.group(1))
            max_page = max(max_page, page_num)

    return max_page


def crawl_hot_page(page_num: int = 1, retry: int = 3) -> Tuple[List[Dict], int]:
    """
    çˆ¬å–æŒ‡å®šé¡µçš„çƒ­é—¨è§†é¢‘æ•°æ®

    Args:
        page_num: é¡µç ï¼ˆä» 1 å¼€å§‹ï¼‰
        retry: é‡è¯•æ¬¡æ•°

    Returns:
        (è§†é¢‘åˆ—è¡¨, æ€»é¡µæ•°)
    """
    if page_num == 1:
        url = 'https://jable.tv/hot/'
    else:
        url = f'https://jable.tv/hot/{page_num}/'

    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")

    try:
        html = fetch_page(url, retry=retry)
        videos = extract_videos_from_page(html)

        # åªåœ¨ç¬¬ä¸€é¡µæ—¶è·å–æ€»é¡µæ•°
        total_pages = get_total_pages(html) if page_num == 1 else 0

        print(f"  âœ“ ç¬¬ {page_num} é¡µçˆ¬å–æˆåŠŸï¼Œæ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")

        return videos, total_pages

    except Exception as e:
        print(f"  âœ— ç¬¬ {page_num} é¡µçˆ¬å–å¤±è´¥: {str(e)[:100]}")
        return [], 0


def crawl_all_hot_pages(start_page: int = 1, end_page: Optional[int] = None,
                       page_delay: Optional[float] = None, task_type: str = 'update',
                       resume: bool = True) -> List[Dict]:
    """
    çˆ¬å–æ‰€æœ‰ï¼ˆæˆ–æŒ‡å®šèŒƒå›´ï¼‰çƒ­é—¨é¡µé¢çš„è§†é¢‘æ•°æ®
    æ”¯æŒæ–­ç‚¹ç»­ä¼ 

    Args:
        start_page: èµ·å§‹é¡µç ï¼ˆé»˜è®¤ 1ï¼‰
        end_page: ç»“æŸé¡µç ï¼ˆNone è¡¨ç¤ºçˆ¬åˆ°æœ€åä¸€é¡µï¼‰
        page_delay: æ¯é¡µä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜åŒ–ç‰ˆ0.5ç§’ï¼ŒåŸç‰ˆ1.0ç§’ï¼‰
        task_type: ä»»åŠ¡ç±»å‹ï¼ˆinit æˆ– updateï¼‰
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤ Trueï¼‰

    Returns:
        æ‰€æœ‰è§†é¢‘çš„åˆ—è¡¨
    """
    all_videos = []
    tracker = ProgressTracker() if resume else None
    task_id = None
    pending_pages = None

    # æ™ºèƒ½å»¶è¿Ÿï¼šä¼˜åŒ–ç‰ˆç”¨0.5ç§’ï¼ŒåŸç‰ˆç”¨1.0ç§’
    if page_delay is None:
        page_delay = 0.5 if USE_FAST_MODE else 1.0

    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆè‡ªåŠ¨æ¢å¤ï¼Œæ— éœ€ç¡®è®¤ï¼‰
    if tracker and resume:
        resume_info = tracker.get_resume_info(task_type)
        if resume_info:
            print("=" * 80)
            print("âœ“ å‘ç°æœªå®Œæˆçš„ä»»åŠ¡ï¼Œè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
            print("=" * 80)
            print(f"  ä»»åŠ¡ID: {resume_info['task_id']}")
            print(f"  å·²å®Œæˆ: {resume_info['completed']}/{resume_info['total']} é¡µ")
            print(f"  è¿›åº¦: {resume_info['progress']:.1f}%")
            print(f"  æœ€åæ›´æ–°: {resume_info['last_update']}")

            if resume_info['failed']:
                print(f"  å¤±è´¥é¡µç : {resume_info['failed']}")

            task_id = resume_info['task_id']
            pending_pages = resume_info['pending']
            print("")

    # å…ˆçˆ¬ç¬¬ä¸€é¡µè·å–æ€»é¡µæ•°
    if not task_id:
        print("\n" + "=" * 80)
        print("å¼€å§‹çˆ¬å–çƒ­é—¨è§†é¢‘æ•°æ®")
        print("=" * 80)

    first_page_videos, total_pages = crawl_hot_page(1, retry=5)

    if total_pages == 0:
        print("âŒ æ— æ³•è·å–æ€»é¡µæ•°ï¼Œçˆ¬å–å¤±è´¥")
        if tracker and task_id:
            tracker.fail_task(task_id, "æ— æ³•è·å–æ€»é¡µæ•°")
        return []

    print(f"\nâœ“ æ€»é¡µæ•°: {total_pages:,}")

    if end_page is None:
        end_page = total_pages
    else:
        end_page = min(end_page, total_pages)

    # åˆ›å»ºæˆ–ç»§ç»­ä»»åŠ¡
    if not task_id and tracker:
        task_id = tracker.start_task(task_type, end_page)

    # ç¡®å®šè¦çˆ¬å–çš„é¡µç åˆ—è¡¨
    if pending_pages is not None:
        # æ–­ç‚¹ç»­ä¼ ï¼šåªçˆ¬æœªå®Œæˆçš„é¡µ
        pages_to_crawl = [p for p in pending_pages if start_page <= p <= end_page]
        print(f"âœ“ æ–­ç‚¹ç»­ä¼ : å‰©ä½™ {len(pages_to_crawl)} é¡µå¾…çˆ¬å–")
    else:
        # æ–°ä»»åŠ¡ï¼šçˆ¬æ‰€æœ‰é¡µ
        pages_to_crawl = list(range(start_page, end_page + 1))
        print(f"âœ“ çˆ¬å–èŒƒå›´: ç¬¬ {start_page} é¡µ åˆ° ç¬¬ {end_page} é¡µ")
        print(f"âœ“ é¢„è®¡è§†é¢‘æ•°é‡: {len(pages_to_crawl) * 24:,} ä¸ª")

    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæ—¶é¢„è®¡è€—æ—¶æ›´å°‘
    avg_time_per_page = 1.5 if USE_FAST_MODE else 5.0
    print(f"âœ“ é¢„è®¡è€—æ—¶: {len(pages_to_crawl) * (avg_time_per_page + page_delay) / 60:.1f} åˆ†é’Ÿ\n")

    # çˆ¬å–é¡µé¢
    total_to_crawl = len(pages_to_crawl)
    for idx, page_num in enumerate(pages_to_crawl, 1):
        try:
            print(f"[{idx}/{total_to_crawl}] ", end="")
            videos, _ = crawl_hot_page(page_num, retry=3)
            all_videos.extend(videos)

            # æ ‡è®°é¡µé¢å®Œæˆ
            if tracker and task_id:
                tracker.update_page(task_id, page_num, success=True)

            # æ˜¾ç¤ºè¿›åº¦
            progress = idx / total_to_crawl * 100
            print(f"  âœ“ è¿›åº¦: {progress:.1f}% | å·²çˆ¬å–: {len(all_videos):,} ä¸ªè§†é¢‘")

        except Exception as e:
            print(f"  âœ— ç¬¬ {page_num} é¡µçˆ¬å–å¤±è´¥: {str(e)[:50]}")
            # æ ‡è®°é¡µé¢å¤±è´¥
            if tracker and task_id:
                tracker.update_page(task_id, page_num, success=False)

        # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
        if idx < total_to_crawl:
            time.sleep(page_delay)

    print("\n" + "=" * 80)
    print(f"âœ“ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_videos):,} ä¸ªè§†é¢‘")
    print("=" * 80)

    # æ ‡è®°ä»»åŠ¡å®Œæˆ
    if tracker and task_id:
        tracker.update_stats(task_id, videos_count=len(all_videos), actors_count=0)
        tracker.complete_task(task_id)

    # æ¸…ç†æµè§ˆå™¨å®ä¾‹ï¼ˆå¦‚æœä½¿ç”¨ä¼˜åŒ–ç‰ˆï¼‰
    if USE_FAST_MODE:
        print("\næ­£åœ¨æ¸…ç†æµè§ˆå™¨å®ä¾‹...")
        cleanup_browser()

    return all_videos


def extract_actors_from_video_page(html: str) -> List[Dict]:
    """
    ä»è§†é¢‘è¯¦æƒ…é¡µæå–æ¼”å‘˜ä¿¡æ¯

    Args:
        html: è§†é¢‘è¯¦æƒ…é¡µ HTML å†…å®¹

    Returns:
        æ¼”å‘˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {actor_id, actor_name}
    """
    # ä½¿ç”¨ lxml è§£æå™¨
    try:
        soup = BeautifulSoup(html, 'lxml')
    except:
        soup = BeautifulSoup(html, 'html.parser')

    actors = []

    # æŸ¥æ‰¾æ¼”å‘˜å®¹å™¨
    models_container = soup.select('div.models a.model')

    for model_link in models_container:
        try:
            # æå–æ¼”å‘˜é“¾æ¥
            href = model_link.get('href', '')
            if not href or '/models/' not in href:
                continue

            # ä»é“¾æ¥ä¸­æå–æ¼”å‘˜ ID
            actor_id = href.split('/')[-2] if '/' in href else ''
            if not actor_id:
                continue

            # æå–æ¼”å‘˜åå­—ï¼ˆä» span çš„ data-original-title å±æ€§ï¼‰
            span_tag = model_link.select_one('span[data-original-title]')
            actor_name = span_tag.get('data-original-title', '') if span_tag else ''

            # å¦‚æœæ²¡æœ‰ data-original-titleï¼Œå°è¯•ä» title å±æ€§è·å–
            if not actor_name:
                actor_name = span_tag.get('title', '') if span_tag else ''

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨æ˜¾ç¤ºçš„æ–‡æœ¬
            if not actor_name:
                actor_name = model_link.get_text(strip=True)

            if actor_id and actor_name:
                actors.append({
                    'actor_id': actor_id,
                    'actor_name': actor_name
                })

        except Exception as e:
            print(f"  âš ï¸  è§£ææ¼”å‘˜å¤±è´¥: {e}")
            continue

    return actors


def crawl_video_actors(video_id: str, video_url: Optional[str] = None, retry: int = 2) -> List[Dict]:
    """
    çˆ¬å–è§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯

    Args:
        video_id: è§†é¢‘ ID
        video_url: è§†é¢‘ URLï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨æ„å»ºï¼‰
        retry: é‡è¯•æ¬¡æ•°

    Returns:
        æ¼”å‘˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {actor_id, actor_name}
    """
    if not video_url:
        video_url = f'https://jable.tv/videos/{video_id}/'

    print(f"  æ­£åœ¨è·å–è§†é¢‘ {video_id} çš„æ¼”å‘˜ä¿¡æ¯...")

    try:
        html = fetch_page(video_url, retry=retry)
        actors = extract_actors_from_video_page(html)

        if actors:
            actor_names = ', '.join([a['actor_name'] for a in actors])
            print(f"    âœ“ æ‰¾åˆ° {len(actors)} ä¸ªæ¼”å‘˜: {actor_names}")
        else:
            print(f"    âš ï¸  æœªæ‰¾åˆ°æ¼”å‘˜ä¿¡æ¯")

        return actors

    except Exception as e:
        print(f"    âœ— è·å–æ¼”å‘˜ä¿¡æ¯å¤±è´¥: {str(e)[:100]}")
        return []


def crawl_multiple_video_actors(video_list: List[Dict], delay: float = 2.0) -> Dict[str, List[Dict]]:
    """
    æ‰¹é‡çˆ¬å–å¤šä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯

    Args:
        video_list: è§†é¢‘åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {video_id, url}
        delay: æ¯ä¸ªè§†é¢‘ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        å­—å…¸ï¼š{video_id: [æ¼”å‘˜åˆ—è¡¨]}
    """
    result = {}
    total = len(video_list)

    print("=" * 80)
    print(f"å¼€å§‹çˆ¬å– {total} ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")
    print("=" * 80)

    for i, video in enumerate(video_list, 1):
        video_id = video['video_id']
        video_url = video.get('url')

        print(f"\n[{i}/{total}] è§†é¢‘: {video_id}")

        actors = crawl_video_actors(video_id, video_url, retry=2)
        result[video_id] = actors

        # å»¶è¿Ÿ
        if i < total:
            time.sleep(delay)

    print("\n" + "=" * 80)
    print(f"âœ“ å®Œæˆï¼å…±è·å– {len(result)} ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")

    # ç»Ÿè®¡
    with_actors = sum(1 for actors in result.values() if actors)
    print(f"  æœ‰æ¼”å‘˜ä¿¡æ¯: {with_actors} ä¸ª")
    print(f"  æ— æ¼”å‘˜ä¿¡æ¯: {total - with_actors} ä¸ª")
    print("=" * 80)

    return result


if __name__ == '__main__':
    # æµ‹è¯•ï¼šçˆ¬å–å‰ 2 é¡µ
    print("æµ‹è¯•çˆ¬è™«æ¨¡å—")
    print("=" * 80)

    # æµ‹è¯•1ï¼šçˆ¬å–å•é¡µ
    print("\nã€æµ‹è¯•1ã€‘çˆ¬å–ç¬¬ 1 é¡µ")
    videos, total_pages = crawl_hot_page(1)
    print(f"âœ“ è·å– {len(videos)} ä¸ªè§†é¢‘")
    print(f"âœ“ æ€»é¡µæ•°: {total_pages}")

    if videos:
        print(f"\nå‰ 3 ä¸ªè§†é¢‘:")
        for i, v in enumerate(videos[:3], 1):
            print(f"  {i}. {v['video_id']:<15} ğŸ‘ï¸  {v['views']:>8,}  ğŸ‘ {v['likes']:>6,}")

    # æµ‹è¯•2ï¼šçˆ¬å–æ¼”å‘˜ä¿¡æ¯
    if videos:
        print(f"\nã€æµ‹è¯•2ã€‘çˆ¬å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")
        first_video = videos[0]
        actors = crawl_video_actors(first_video['video_id'], first_video['url'])
        print(f"âœ“ æ¼”å‘˜æ•°é‡: {len(actors)}")
        for actor in actors:
            print(f"  - {actor['actor_name']} ({actor['actor_id']})")

    print("\nâœ“ çˆ¬è™«æ¨¡å—æµ‹è¯•å®Œæˆ")
