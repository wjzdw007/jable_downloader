#!/usr/bin/env python3
"""
çƒ­é—¨å½±ç‰‡æ•°æ®çˆ¬è™«æ¨¡å—
è´Ÿè´£ä» https://jable.tv/hot/ çˆ¬å–è§†é¢‘æ•°æ®å’Œæ¼”å‘˜ä¿¡æ¯
"""

import re
import time
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup

# ä½¿ç”¨ä¼˜åŒ–ç‰ˆçš„ utilsï¼ˆå¤ç”¨æµè§ˆå™¨ï¼Œç¦ç”¨èµ„æºï¼Œç§»é™¤å›ºå®šç­‰å¾…ï¼‰
try:
    import utils_fast
    USE_FAST_MODE = True
    print("â„¹ï¸  ä½¿ç”¨ä¼˜åŒ–ç‰ˆçˆ¬è™«ï¼ˆutils_fastï¼‰")

    def fetch_page(url: str, retry: int = 3) -> str:
        """ç»Ÿä¸€çš„é¡µé¢è·å–æ¥å£ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        return utils_fast.fast_requests_get(url, retry)

    def cleanup_browser():
        """æ¸…ç†æµè§ˆå™¨å®ä¾‹"""
        utils_fast.close_browser_instance()

except ImportError:
    import utils
    USE_FAST_MODE = False
    print("âš ï¸  ä¼˜åŒ–ç‰ˆä¸å¯ç”¨ï¼Œä½¿ç”¨åŸç‰ˆ utils")

    def fetch_page(url: str, retry: int = 3) -> str:
        """ç»Ÿä¸€çš„é¡µé¢è·å–æ¥å£ï¼ˆåŸç‰ˆï¼‰"""
        return utils.scrapingant_requests_get(url, retry)

    def cleanup_browser():
        """æ¸…ç†æµè§ˆå™¨å®ä¾‹ï¼ˆåŸç‰ˆæ— éœ€æ¸…ç†ï¼‰"""
        pass


def extract_videos_from_page(html: str) -> List[Dict]:
    """
    ä»é¡µé¢ HTML ä¸­æå–è§†é¢‘ä¿¡æ¯

    Args:
        html: é¡µé¢ HTML å†…å®¹

    Returns:
        è§†é¢‘åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {video_id, title, views, likes, url}
    """
    soup = BeautifulSoup(html, 'html.parser')
    video_containers = soup.select('div.video-img-box')

    videos = []

    for container in video_containers:
        try:
            # æå–è§†é¢‘é“¾æ¥å’Œ ID
            link_tag = container.select_one('a[href*="/videos/"]')
            if not link_tag:
                continue

            video_url = link_tag.get('href', '')
            if not video_url:
                continue

            # ä» URL ä¸­æå–è§†é¢‘ ID
            video_id = video_url.split('/')[-2] if '/' in video_url else ''
            if not video_id:
                continue

            # æå–æ ‡é¢˜
            title_tag = container.select_one('h6.title a')
            title = title_tag.get_text(strip=True) if title_tag else ''

            # æå–ç»Ÿè®¡æ•°æ®ï¼ˆè§‚çœ‹æ•°å’Œç‚¹èµæ•°ï¼‰
            sub_title = container.select_one('p.sub-title')

            views = 0
            likes = 0

            if sub_title:
                text = sub_title.get_text()

                # æŒ‰è¡Œåˆ†å‰²ï¼Œæå–æ¯è¡Œçš„æ•°å­—
                lines = text.strip().split('\n')
                numbers = []

                for line in lines:
                    line = line.strip()
                    if line:
                        # å»é™¤æ‰€æœ‰ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦
                        num_str = re.sub(r'\s+', '', line)
                        if num_str.isdigit():
                            numbers.append(int(num_str))

                # ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯è§‚çœ‹æ•°ï¼Œç¬¬äºŒä¸ªæ˜¯ç‚¹èµæ•°
                if len(numbers) >= 2:
                    views = numbers[0]
                    likes = numbers[1]

            videos.append({
                'video_id': video_id,
                'title': title,
                'views': views,
                'likes': likes,
                'url': video_url
            })

        except Exception as e:
            print(f"âš ï¸  è§£æè§†é¢‘å¤±è´¥: {e}")
            continue

    return videos


def get_total_pages(html: str) -> int:
    """
    ä»é¡µé¢ HTML ä¸­è·å–æ€»é¡µæ•°

    Args:
        html: é¡µé¢ HTML å†…å®¹

    Returns:
        æ€»é¡µæ•°
    """
    soup = BeautifulSoup(html, 'html.parser')

    # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥
    pagination_links = soup.select('ul.pagination li a[href*="/hot/"]')
    max_page = 1

    for link in pagination_links:
        # æ£€æŸ¥æ˜¯å¦æ˜¯"æœ€å¾Œ"é“¾æ¥
        text = link.get_text(strip=True)
        href = link.get('href', '')

        # å¦‚æœæ˜¯"æœ€å¾Œ"é“¾æ¥ï¼Œä¼˜å…ˆä½¿ç”¨
        if 'æœ€å¾Œ' in text or 'Â»' in text:
            match = re.search(r'/hot/(\d+)/', href)
            if match:
                return int(match.group(1))

        # å¦åˆ™ï¼Œè®°å½•æœ€å¤§é¡µç 
        match = re.search(r'/hot/(\d+)/', href)
        if match:
            page_num = int(match.group(1))
            max_page = max(max_page, page_num)

    return max_page


def crawl_hot_page(page_num: int = 1, retry: int = 3) -> Tuple[List[Dict], int]:
    """
    çˆ¬å–æŒ‡å®šé¡µçš„çƒ­é—¨è§†é¢‘æ•°æ®

    Args:
        page_num: é¡µç ï¼ˆä» 1 å¼€å§‹ï¼‰
        retry: é‡è¯•æ¬¡æ•°

    Returns:
        (è§†é¢‘åˆ—è¡¨, æ€»é¡µæ•°)
    """
    if page_num == 1:
        url = 'https://jable.tv/hot/'
    else:
        url = f'https://jable.tv/hot/{page_num}/'

    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")

    try:
        html = fetch_page(url, retry=retry)
        videos = extract_videos_from_page(html)

        # åªåœ¨ç¬¬ä¸€é¡µæ—¶è·å–æ€»é¡µæ•°
        total_pages = get_total_pages(html) if page_num == 1 else 0

        print(f"  âœ“ ç¬¬ {page_num} é¡µçˆ¬å–æˆåŠŸï¼Œæ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")

        return videos, total_pages

    except Exception as e:
        print(f"  âœ— ç¬¬ {page_num} é¡µçˆ¬å–å¤±è´¥: {str(e)[:100]}")
        return [], 0


def crawl_all_hot_pages(start_page: int = 1, end_page: Optional[int] = None,
                       page_delay: float = 1.0) -> List[Dict]:
    """
    çˆ¬å–æ‰€æœ‰ï¼ˆæˆ–æŒ‡å®šèŒƒå›´ï¼‰çƒ­é—¨é¡µé¢çš„è§†é¢‘æ•°æ®

    Args:
        start_page: èµ·å§‹é¡µç ï¼ˆé»˜è®¤ 1ï¼‰
        end_page: ç»“æŸé¡µç ï¼ˆNone è¡¨ç¤ºçˆ¬åˆ°æœ€åä¸€é¡µï¼‰
        page_delay: æ¯é¡µä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        æ‰€æœ‰è§†é¢‘çš„åˆ—è¡¨
    """
    all_videos = []

    # å…ˆçˆ¬ç¬¬ä¸€é¡µè·å–æ€»é¡µæ•°
    print("=" * 80)
    print("å¼€å§‹çˆ¬å–çƒ­é—¨è§†é¢‘æ•°æ®")
    print("=" * 80)

    first_page_videos, total_pages = crawl_hot_page(1, retry=5)

    if total_pages == 0:
        print("âŒ æ— æ³•è·å–æ€»é¡µæ•°ï¼Œçˆ¬å–å¤±è´¥")
        return []

    print(f"\nâœ“ æ€»é¡µæ•°: {total_pages:,}")

    if end_page is None:
        end_page = total_pages
    else:
        end_page = min(end_page, total_pages)

    # å¦‚æœèµ·å§‹é¡µæ˜¯ 1ï¼Œä½¿ç”¨å·²ç»çˆ¬å–çš„æ•°æ®
    if start_page == 1:
        all_videos.extend(first_page_videos)
        start_page = 2

    print(f"âœ“ çˆ¬å–èŒƒå›´: ç¬¬ {start_page} é¡µ åˆ° ç¬¬ {end_page} é¡µ")
    print(f"âœ“ é¢„è®¡è§†é¢‘æ•°é‡: {(end_page - start_page + 1) * 24:,} ä¸ª")
    print(f"âœ“ é¢„è®¡è€—æ—¶: {(end_page - start_page + 1) * (3 + page_delay) / 60:.1f} åˆ†é’Ÿ\n")

    # çˆ¬å–å‰©ä½™é¡µé¢
    for page_num in range(start_page, end_page + 1):
        videos, _ = crawl_hot_page(page_num, retry=3)
        all_videos.extend(videos)

        # æ˜¾ç¤ºè¿›åº¦
        progress = (page_num - start_page + 1) / (end_page - start_page + 1) * 100
        print(f"  è¿›åº¦: {progress:.1f}% ({page_num}/{end_page}) | å·²çˆ¬å–: {len(all_videos):,} ä¸ªè§†é¢‘")

        # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
        if page_num < end_page:
            time.sleep(page_delay)

    print("\n" + "=" * 80)
    print(f"âœ“ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_videos):,} ä¸ªè§†é¢‘")
    print("=" * 80)

    # æ¸…ç†æµè§ˆå™¨å®ä¾‹ï¼ˆå¦‚æœä½¿ç”¨ä¼˜åŒ–ç‰ˆï¼‰
    if USE_FAST_MODE:
        print("\næ­£åœ¨æ¸…ç†æµè§ˆå™¨å®ä¾‹...")
        cleanup_browser()

    return all_videos


def extract_actors_from_video_page(html: str) -> List[Dict]:
    """
    ä»è§†é¢‘è¯¦æƒ…é¡µæå–æ¼”å‘˜ä¿¡æ¯

    Args:
        html: è§†é¢‘è¯¦æƒ…é¡µ HTML å†…å®¹

    Returns:
        æ¼”å‘˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {actor_id, actor_name}
    """
    soup = BeautifulSoup(html, 'html.parser')

    actors = []

    # æŸ¥æ‰¾æ¼”å‘˜å®¹å™¨
    models_container = soup.select('div.models a.model')

    for model_link in models_container:
        try:
            # æå–æ¼”å‘˜é“¾æ¥
            href = model_link.get('href', '')
            if not href or '/models/' not in href:
                continue

            # ä»é“¾æ¥ä¸­æå–æ¼”å‘˜ ID
            actor_id = href.split('/')[-2] if '/' in href else ''
            if not actor_id:
                continue

            # æå–æ¼”å‘˜åå­—ï¼ˆä» span çš„ data-original-title å±æ€§ï¼‰
            span_tag = model_link.select_one('span[data-original-title]')
            actor_name = span_tag.get('data-original-title', '') if span_tag else ''

            # å¦‚æœæ²¡æœ‰ data-original-titleï¼Œå°è¯•ä» title å±æ€§è·å–
            if not actor_name:
                actor_name = span_tag.get('title', '') if span_tag else ''

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨æ˜¾ç¤ºçš„æ–‡æœ¬
            if not actor_name:
                actor_name = model_link.get_text(strip=True)

            if actor_id and actor_name:
                actors.append({
                    'actor_id': actor_id,
                    'actor_name': actor_name
                })

        except Exception as e:
            print(f"  âš ï¸  è§£ææ¼”å‘˜å¤±è´¥: {e}")
            continue

    return actors


def crawl_video_actors(video_id: str, video_url: Optional[str] = None, retry: int = 2) -> List[Dict]:
    """
    çˆ¬å–è§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯

    Args:
        video_id: è§†é¢‘ ID
        video_url: è§†é¢‘ URLï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨æ„å»ºï¼‰
        retry: é‡è¯•æ¬¡æ•°

    Returns:
        æ¼”å‘˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {actor_id, actor_name}
    """
    if not video_url:
        video_url = f'https://jable.tv/videos/{video_id}/'

    print(f"  æ­£åœ¨è·å–è§†é¢‘ {video_id} çš„æ¼”å‘˜ä¿¡æ¯...")

    try:
        html = fetch_page(video_url, retry=retry)
        actors = extract_actors_from_video_page(html)

        if actors:
            actor_names = ', '.join([a['actor_name'] for a in actors])
            print(f"    âœ“ æ‰¾åˆ° {len(actors)} ä¸ªæ¼”å‘˜: {actor_names}")
        else:
            print(f"    âš ï¸  æœªæ‰¾åˆ°æ¼”å‘˜ä¿¡æ¯")

        return actors

    except Exception as e:
        print(f"    âœ— è·å–æ¼”å‘˜ä¿¡æ¯å¤±è´¥: {str(e)[:100]}")
        return []


def crawl_multiple_video_actors(video_list: List[Dict], delay: float = 2.0) -> Dict[str, List[Dict]]:
    """
    æ‰¹é‡çˆ¬å–å¤šä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯

    Args:
        video_list: è§†é¢‘åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {video_id, url}
        delay: æ¯ä¸ªè§†é¢‘ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        å­—å…¸ï¼š{video_id: [æ¼”å‘˜åˆ—è¡¨]}
    """
    result = {}
    total = len(video_list)

    print("=" * 80)
    print(f"å¼€å§‹çˆ¬å– {total} ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")
    print("=" * 80)

    for i, video in enumerate(video_list, 1):
        video_id = video['video_id']
        video_url = video.get('url')

        print(f"\n[{i}/{total}] è§†é¢‘: {video_id}")

        actors = crawl_video_actors(video_id, video_url, retry=2)
        result[video_id] = actors

        # å»¶è¿Ÿ
        if i < total:
            time.sleep(delay)

    print("\n" + "=" * 80)
    print(f"âœ“ å®Œæˆï¼å…±è·å– {len(result)} ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")

    # ç»Ÿè®¡
    with_actors = sum(1 for actors in result.values() if actors)
    print(f"  æœ‰æ¼”å‘˜ä¿¡æ¯: {with_actors} ä¸ª")
    print(f"  æ— æ¼”å‘˜ä¿¡æ¯: {total - with_actors} ä¸ª")
    print("=" * 80)

    return result


if __name__ == '__main__':
    # æµ‹è¯•ï¼šçˆ¬å–å‰ 2 é¡µ
    print("æµ‹è¯•çˆ¬è™«æ¨¡å—")
    print("=" * 80)

    # æµ‹è¯•1ï¼šçˆ¬å–å•é¡µ
    print("\nã€æµ‹è¯•1ã€‘çˆ¬å–ç¬¬ 1 é¡µ")
    videos, total_pages = crawl_hot_page(1)
    print(f"âœ“ è·å– {len(videos)} ä¸ªè§†é¢‘")
    print(f"âœ“ æ€»é¡µæ•°: {total_pages}")

    if videos:
        print(f"\nå‰ 3 ä¸ªè§†é¢‘:")
        for i, v in enumerate(videos[:3], 1):
            print(f"  {i}. {v['video_id']:<15} ğŸ‘ï¸  {v['views']:>8,}  ğŸ‘ {v['likes']:>6,}")

    # æµ‹è¯•2ï¼šçˆ¬å–æ¼”å‘˜ä¿¡æ¯
    if videos:
        print(f"\nã€æµ‹è¯•2ã€‘çˆ¬å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„æ¼”å‘˜ä¿¡æ¯")
        first_video = videos[0]
        actors = crawl_video_actors(first_video['video_id'], first_video['url'])
        print(f"âœ“ æ¼”å‘˜æ•°é‡: {len(actors)}")
        for actor in actors:
            print(f"  - {actor['actor_name']} ({actor['actor_id']})")

    print("\nâœ“ çˆ¬è™«æ¨¡å—æµ‹è¯•å®Œæˆ")
